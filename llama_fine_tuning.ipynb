{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":53383,"status":"ok","timestamp":1764294784412,"user":{"displayName":"Paul Kilian","userId":"12769555656558612045"},"user_tz":-60},"id":"2eSvM9zX_2d3"},"outputs":[],"source":["%%capture\n","!pip install unsloth optuna\n","# Also get the latest nightly Unsloth!\n","!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git@nightly git+https://github.com/unslothai/unsloth-zoo.git"]},{"cell_type":"markdown","metadata":{"id":"r2v_X2fA0Df5"},"source":["* We support Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc\n","* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.\n","* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.\n","* [**NEW**] We make Gemma-2 9b / 27b **2x faster**! See our [Gemma-2 9b notebook](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)\n","* [**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53825,"status":"ok","timestamp":1764294838235,"user":{"displayName":"Paul Kilian","userId":"12769555656558612045"},"user_tz":-60},"id":"QmUBVEnvCDJv","outputId":"21492e30-6306-450a-f4bf-8ad4446d7705"},"outputs":[{"output_type":"stream","name":"stdout","text":["ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","ü¶• Unsloth Zoo will now patch everything to make training faster!\n","==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]}],"source":["from unsloth import FastLanguageModel\n","import torch\n","max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n","dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n","\n","# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n","fourbit_models = [\n","    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n","    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n","    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n","    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n","    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n","    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n","    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n","    \"unsloth/Phi-3-medium-4k-instruct\",\n","    \"unsloth/gemma-2-9b-bnb-4bit\",\n","    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n","\n","    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n","    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n","    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n","    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n","\n","    \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # NEW! Llama 3.3 70B!\n","] # More models at https://huggingface.co/unsloth\n","\n","base_model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n","    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1796,"status":"ok","timestamp":1764294840032,"user":{"displayName":"Paul Kilian","userId":"12769555656558612045"},"user_tz":-60},"id":"AUKgG67SHrCw","outputId":"5141aea9-9e0b-4dc7-e176-8ab0bbabd77e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","import os\n","\n","drive.mount('/content/drive')\n","checkpoint_dir = '/content/drive/MyDrive/llama-3b-lora-checkpoints'\n","os.makedirs(checkpoint_dir, exist_ok=True)"]},{"cell_type":"markdown","source":["## Data Preparations"],"metadata":{"id":"psn_AcDVAUuQ"}},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1764294840050,"user":{"displayName":"Paul Kilian","userId":"12769555656558612045"},"user_tz":-60},"id":"xrkQupxvIZ_i"},"outputs":[],"source":["from unsloth.chat_templates import get_chat_template\n","\n","tokenizer = get_chat_template(\n","    tokenizer,\n","    chat_template = \"llama-3\",\n","    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n","    map_eos_token = True,        # Maps <|im_end|> to <|eot_id|> instead\n",")\n","\n","def general_formatting_prompts_func(examples):\n","    convos = examples[\"conversations\"]\n","    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n","    return { \"text\" : texts, }\n","\n","def tool_formatting_prompts_func(examples):\n","    convos = []\n","\n","    # Iterate through each item in the batch (examples are structured as lists of values)\n","    for query, tools, answers in zip(examples['query'], examples['tools'], examples['answers']):\n","        tool_user = {\n","            \"content\": f\"You are a helpful assistant with access to the following tools or function calls. Your task is to produce a sequence of tools or function calls necessary to generate response to the user utterance. Use the following tools or function calls as required:\\n{tools}\",\n","            \"role\": \"system\"\n","        }\n","        ques_user = {\n","            \"content\": f\"{query}\",\n","            \"role\": \"user\"\n","        }\n","        assistant = {\n","            \"content\": f\"{answers}\",\n","            \"role\": \"assistant\"\n","        }\n","        convos.append([tool_user, ques_user, assistant])\n","\n","    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n","    return {\"text\": texts}"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6466,"status":"ok","timestamp":1764294846528,"user":{"displayName":"Paul Kilian","userId":"12769555656558612045"},"user_tz":-60},"id":"X4XxF43YI5GJ","outputId":"52bc0196-093a-4df0-bb5f-2e310f6c384c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using a sample size of 50000 for general fine-tuning.\n","Using a sample size of 25000 for tool fine-tuning.\n","Using a total dataset of 75000 for fine-tuning.\n"]}],"source":["from unsloth.chat_templates import standardize_sharegpt\n","from datasets import load_dataset, concatenate_datasets\n","from huggingface_hub import login\n","from google.colab import userdata\n","\n","hf_token = userdata.get('HF_TOKEN')\n","\n","if hf_token is None:\n","    raise EnvironmentError(\"HF_TOKEN is not set in the environment variables.\")\n","login(hf_token)\n","\n","general_dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")\n","general_dataset = general_dataset.select(range(50000))\n","general_dataset = standardize_sharegpt(general_dataset)\n","general_dataset = general_dataset.map(general_formatting_prompts_func, batched = True)\n","print(f\"Using a sample size of {len(general_dataset)} for general fine-tuning.\")\n","\n","tool_dataset = load_dataset(\"Salesforce/xlam-function-calling-60k\", split=\"train\", token=hf_token)\n","tool_dataset = tool_dataset.select(range(25000))\n","tool_dataset = tool_dataset.map(tool_formatting_prompts_func, batched = True)\n","print(f\"Using a sample size of {len(tool_dataset)} for tool fine-tuning.\")\n","\n","dataset = concatenate_datasets([general_dataset, tool_dataset])\n","print(f\"Using a total dataset of {len(dataset)} for fine-tuning.\")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1764294846541,"user":{"displayName":"Paul Kilian","userId":"12769555656558612045"},"user_tz":-60},"id":"4yPCETNBLOwQ","outputId":"ae580524-1ac4-42f8-fb1d-5dd93e131d96"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'conversations': [{'content': 'How do astronomers determine the original wavelength of light emitted by a celestial body at rest, which is necessary for measuring its speed using the Doppler effect?',\n","   'role': 'user'},\n","  {'content': 'Astronomers make use of the unique spectral fingerprints of elements found in stars. These elements emit and absorb light at specific, known wavelengths, forming an absorption spectrum. By analyzing the light received from distant stars and comparing it to the laboratory-measured spectra of these elements, astronomers can identify the shifts in these wavelengths due to the Doppler effect. The observed shift tells them the extent to which the light has been redshifted or blueshifted, thereby allowing them to calculate the speed of the star along the line of sight relative to Earth.',\n","   'role': 'assistant'}],\n"," 'source': 'WebInstructSub_axolotl',\n"," 'score': 5.025244235992432,\n"," 'text': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nHow do astronomers determine the original wavelength of light emitted by a celestial body at rest, which is necessary for measuring its speed using the Doppler effect?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAstronomers make use of the unique spectral fingerprints of elements found in stars. These elements emit and absorb light at specific, known wavelengths, forming an absorption spectrum. By analyzing the light received from distant stars and comparing it to the laboratory-measured spectra of these elements, astronomers can identify the shifts in these wavelengths due to the Doppler effect. The observed shift tells them the extent to which the light has been redshifted or blueshifted, thereby allowing them to calculate the speed of the star along the line of sight relative to Earth.<|eot_id|>',\n"," 'id': None,\n"," 'query': None,\n"," 'answers': None,\n"," 'tools': None}"]},"metadata":{},"execution_count":6}],"source":["dataset[5]"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1764294846552,"user":{"displayName":"Paul Kilian","userId":"12769555656558612045"},"user_tz":-60},"id":"8l0hhQ3gLQH_","outputId":"9796a7f4-4bd7-4aea-eea8-ce0f23a34ded"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'conversations': None,\n"," 'source': None,\n"," 'score': None,\n"," 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant with access to the following tools or function calls. Your task is to produce a sequence of tools or function calls necessary to generate response to the user utterance. Use the following tools or function calls as required:\\n[{\"name\": \"email_extractor\", \"description\": \"Extracts emails from the provided URL using the RapidAPI email scraper service.\", \"parameters\": {\"url\": {\"description\": \"The URL from which to extract emails.\", \"type\": \"str\", \"default\": \"https://en.wikipedia.org/wiki/Email\"}}}]<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nCan you help me find emails from a website about sustainable fashion? Also, I need emails from a tech blog that talks about AI and machine learning. And could you also extract emails from a local bakery\\'s website?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n[{\"name\": \"email_extractor\", \"arguments\": {\"url\": \"https://www.sustainablefashionblog.com\"}}, {\"name\": \"email_extractor\", \"arguments\": {\"url\": \"https://www.techblogai.com\"}}, {\"name\": \"email_extractor\", \"arguments\": {\"url\": \"https://www.localbakerywebsite.com\"}}]<|eot_id|>',\n"," 'id': 24995,\n"," 'query': \"Can you help me find emails from a website about sustainable fashion? Also, I need emails from a tech blog that talks about AI and machine learning. And could you also extract emails from a local bakery's website?\",\n"," 'answers': '[{\"name\": \"email_extractor\", \"arguments\": {\"url\": \"https://www.sustainablefashionblog.com\"}}, {\"name\": \"email_extractor\", \"arguments\": {\"url\": \"https://www.techblogai.com\"}}, {\"name\": \"email_extractor\", \"arguments\": {\"url\": \"https://www.localbakerywebsite.com\"}}]',\n"," 'tools': '[{\"name\": \"email_extractor\", \"description\": \"Extracts emails from the provided URL using the RapidAPI email scraper service.\", \"parameters\": {\"url\": {\"description\": \"The URL from which to extract emails.\", \"type\": \"str\", \"default\": \"https://en.wikipedia.org/wiki/Email\"}}}]'}"]},"metadata":{},"execution_count":7}],"source":["dataset[-5]"]},{"cell_type":"markdown","source":["## Hyperparameter Search"],"metadata":{"id":"F97hxC5jAbrG"}},{"cell_type":"markdown","metadata":{"id":"SXd9bTZd1aaL"},"source":["### Coarse-grained Search"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"GTYyWVZIuzDq","collapsed":true,"executionInfo":{"status":"ok","timestamp":1764264628483,"user_tz":-60,"elapsed":22772291,"user":{"displayName":"Paul Kilian","userId":"12769555656558612045"}},"outputId":"09564164-c5ed-44b6-b82d-84568ffd54a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Starting Deterministic Fixed-Budget Search: 16 Trials at 100 Steps Each\n","============================================================\n","\n","--- Starting Trial 1 ---\n","Params: LR=4.28e-05, r=8, alpha=16, accum=2\n","==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["The model is already on multiple devices. Skipping the move to device specified in `args`.\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 74,250 | Num Epochs = 1 | Total steps = 100\n","O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 2\n","\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 2 x 1) = 16\n"," \"-____-\"     Trainable parameters = 5,636,096 of 1,241,450,496 (0.45% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 10:06, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>0.950900</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.922700</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.901400</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.947400</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.863200</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.838800</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.946900</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.077000</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.887500</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.886800</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.918000</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.922500</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.928900</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.941900</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.958700</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.979200</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.758600</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.876200</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.923700</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.773400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [188/188 02:25]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loss for Trial 1: 0.8473519682884216\n","üèÜ NEW BEST LOSS FOUND: 0.8474 at Trial 1\n","\n","--- Starting Trial 2 ---\n","Params: LR=3.40e-05, r=8, alpha=16, accum=4\n","==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["The model is already on multiple devices. Skipping the move to device specified in `args`.\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 74,250 | Num Epochs = 1 | Total steps = 100\n","O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n","\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n"," \"-____-\"     Trainable parameters = 5,636,096 of 1,241,450,496 (0.45% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 19:59, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>0.960000</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.936600</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.905300</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.048000</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.918700</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.951800</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.961600</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.984200</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.841600</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.865000</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.944200</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.900700</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.830800</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.846200</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.927600</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.886500</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.929700</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.839000</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.856600</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.847200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [188/188 02:25]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loss for Trial 2: 0.8476158976554871\n","\n","--- Starting Trial 3 ---\n","Params: LR=2.24e-05, r=8, alpha=16, accum=2\n","==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["The model is already on multiple devices. Skipping the move to device specified in `args`.\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 74,250 | Num Epochs = 1 | Total steps = 100\n","O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 2\n","\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 2 x 1) = 16\n"," \"-____-\"     Trainable parameters = 5,636,096 of 1,241,450,496 (0.45% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 10:06, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>0.951400</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.929200</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.911800</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.963900</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.881400</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.858600</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.970700</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.098900</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.914700</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.912800</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.940500</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.948700</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.951200</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.965100</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.980100</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>1.003500</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.782600</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.896200</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.942000</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.799400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [188/188 02:25]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loss for Trial 3: 0.870863676071167\n","\n","--- Starting Trial 4 ---\n","Params: LR=6.19e-05, r=16, alpha=16, accum=4\n","==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["The model is already on multiple devices. Skipping the move to device specified in `args`.\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 74,250 | Num Epochs = 1 | Total steps = 100\n","O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n","\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n"," \"-____-\"     Trainable parameters = 11,272,192 of 1,247,086,592 (0.90% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 20:03, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>0.959500</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.930100</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.893300</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.030100</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.896900</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.931300</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.940400</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.964000</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.821900</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.845900</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.925000</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.879600</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.815200</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.830700</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.911700</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.869700</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.912800</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.820600</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.841300</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.829300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [188/188 02:25]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loss for Trial 4: 0.830998957157135\n","üèÜ NEW BEST LOSS FOUND: 0.8310 at Trial 4\n","\n","--- Starting Trial 5 ---\n","Params: LR=1.08e-04, r=8, alpha=16, accum=2\n","==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["The model is already on multiple devices. Skipping the move to device specified in `args`.\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 74,250 | Num Epochs = 1 | Total steps = 100\n","O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 2\n","\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 2 x 1) = 16\n"," \"-____-\"     Trainable parameters = 5,636,096 of 1,241,450,496 (0.45% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 10:06, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>0.948700</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.908700</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.875400</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.914300</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.836000</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.817800</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.916100</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.051900</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.857200</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.858400</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.894900</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.899100</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.907600</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.920400</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.937700</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.959500</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.732800</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.857600</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.904100</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.748900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [188/188 02:25]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loss for Trial 5: 0.8257739543914795\n","üèÜ NEW BEST LOSS FOUND: 0.8258 at Trial 5\n","\n","--- Starting Trial 6 ---\n","Params: LR=4.06e-05, r=16, alpha=16, accum=2\n","==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["The model is already on multiple devices. Skipping the move to device specified in `args`.\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 74,250 | Num Epochs = 1 | Total steps = 100\n","O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 2\n","\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 2 x 1) = 16\n"," \"-____-\"     Trainable parameters = 11,272,192 of 1,247,086,592 (0.90% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 10:08, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>0.951100</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.924200</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.903400</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.950400</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.866100</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.841400</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.950800</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.079900</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.891300</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.889700</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.920400</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.925100</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.931100</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.943700</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.960700</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.981300</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.760800</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.878000</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.925000</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.775400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [188/188 02:25]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loss for Trial 6: 0.849261462688446\n","\n","--- Starting Trial 7 ---\n","Params: LR=3.81e-05, r=16, alpha=16, accum=4\n","==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["The model is already on multiple devices. Skipping the move to device specified in `args`.\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 74,250 | Num Epochs = 1 | Total steps = 100\n","O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n","\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n"," \"-____-\"     Trainable parameters = 11,272,192 of 1,247,086,592 (0.90% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 20:04, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>0.960000</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.936200</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.904300</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.046400</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.916100</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.949200</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.958600</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.980900</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.838300</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.861400</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.940900</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.897000</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.827900</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.843300</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.924700</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.883400</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.926700</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.835700</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.853900</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.844000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [188/188 02:25]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loss for Trial 7: 0.8444796800613403\n","\n","--- Starting Trial 8 ---\n","Params: LR=4.58e-05, r=16, alpha=32, accum=2\n","==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["The model is already on multiple devices. Skipping the move to device specified in `args`.\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 74,250 | Num Epochs = 1 | Total steps = 100\n","O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 2\n","\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 2 x 1) = 16\n"," \"-____-\"     Trainable parameters = 11,272,192 of 1,247,086,592 (0.90% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 10:09, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>0.949500</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.913500</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.886600</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.927200</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.847800</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.825200</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.928000</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.062100</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.869100</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.870300</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.904600</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.908200</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.916600</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.929300</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.946500</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.967700</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.743600</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.865800</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.912100</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.759500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [188/188 02:25]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loss for Trial 8: 0.8348084092140198\n","\n","--- Starting Trial 9 ---\n","Params: LR=1.05e-04, r=16, alpha=32, accum=8\n","==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["The model is already on multiple devices. Skipping the move to device specified in `args`.\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 74,250 | Num Epochs = 1 | Total steps = 100\n","O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n","\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n"," \"-____-\"     Trainable parameters = 11,272,192 of 1,247,086,592 (0.90% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 40:00, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>0.949100</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.969200</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.916500</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.946400</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.835600</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.897600</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.815300</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.878000</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.858200</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.817300</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.837700</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.940500</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.874900</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.830900</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.887200</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.839300</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.861500</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.840400</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.829800</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.838700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [188/188 02:25]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loss for Trial 9: 0.8058328628540039\n","üèÜ NEW BEST LOSS FOUND: 0.8058 at Trial 9\n","\n","--- Starting Trial 10 ---\n","Params: LR=1.22e-04, r=32, alpha=32, accum=8\n","==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["The model is already on multiple devices. Skipping the move to device specified in `args`.\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 74,250 | Num Epochs = 1 | Total steps = 100\n","O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n","\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n"," \"-____-\"     Trainable parameters = 22,544,384 of 1,258,358,784 (1.79% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 40:25, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>0.948400</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.965300</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.911700</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.943700</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.831700</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.894300</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.812900</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.875300</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.855500</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.814700</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.834600</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.937500</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.871800</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.828300</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.884500</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.836900</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.858900</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.837600</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.827000</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.836000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [188/188 02:26]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loss for Trial 10: 0.8030117750167847\n","üèÜ NEW BEST LOSS FOUND: 0.8030 at Trial 10\n","\n","--- Starting Trial 11 ---\n","Params: LR=1.46e-04, r=16, alpha=32, accum=8\n","==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["The model is already on multiple devices. Skipping the move to device specified in `args`.\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 74,250 | Num Epochs = 1 | Total steps = 100\n","O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n","\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n"," \"-____-\"     Trainable parameters = 11,272,192 of 1,247,086,592 (0.90% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 40:01, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>0.947100</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.960400</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.906100</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.940500</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.827800</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.890500</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.809800</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.871800</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.851900</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.811900</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.831500</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.934300</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.868600</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.825500</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.881600</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.834400</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.855800</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.833900</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.823400</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.832800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [188/188 02:25]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loss for Trial 11: 0.7997848987579346\n","üèÜ NEW BEST LOSS FOUND: 0.7998 at Trial 11\n","\n","--- Starting Trial 12 ---\n","Params: LR=2.66e-05, r=8, alpha=8, accum=2\n","==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["The model is already on multiple devices. Skipping the move to device specified in `args`.\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 74,250 | Num Epochs = 1 | Total steps = 100\n","O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 2\n","\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 2 x 1) = 16\n"," \"-____-\"     Trainable parameters = 5,636,096 of 1,241,450,496 (0.45% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 10:06, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>0.951600</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.932100</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.917700</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.971200</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.889000</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.868100</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.981000</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.108600</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.926000</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.923300</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.949600</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.958900</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.960400</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.974900</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.989200</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>1.015000</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.792600</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.905000</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.950000</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.811100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [188/188 02:25]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loss for Trial 12: 0.8811953067779541\n","\n","--- Starting Trial 13 ---\n","Params: LR=2.88e-05, r=8, alpha=16, accum=4\n","==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["The model is already on multiple devices. Skipping the move to device specified in `args`.\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 74,250 | Num Epochs = 1 | Total steps = 100\n","O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n","\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n"," \"-____-\"     Trainable parameters = 5,636,096 of 1,241,450,496 (0.45% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 20:04, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>0.960100</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.938300</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.908200</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.052200</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.924700</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.957400</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.967500</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.991000</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.847900</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.871700</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.951100</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.908000</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.836400</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.851900</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.933400</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.891900</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.935200</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.844900</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.861800</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.852900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [188/188 02:25]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loss for Trial 13: 0.853173017501831\n","\n","--- Starting Trial 14 ---\n","Params: LR=3.18e-05, r=16, alpha=32, accum=8\n","==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["The model is already on multiple devices. Skipping the move to device specified in `args`.\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 74,250 | Num Epochs = 1 | Total steps = 100\n","O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n","\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n"," \"-____-\"     Trainable parameters = 11,272,192 of 1,247,086,592 (0.90% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 40:03, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>0.953200</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.991500</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.953800</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.983300</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.870800</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.935000</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.843300</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.907500</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.889000</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.845600</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.864800</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.969900</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.903300</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.857200</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.912600</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.862000</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.886500</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.868800</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.856000</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.865100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [188/188 02:25]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loss for Trial 14: 0.8320459723472595\n","\n","--- Starting Trial 15 ---\n","Params: LR=1.67e-04, r=16, alpha=32, accum=4\n","==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["The model is already on multiple devices. Skipping the move to device specified in `args`.\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 74,250 | Num Epochs = 1 | Total steps = 100\n","O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n","\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n"," \"-____-\"     Trainable parameters = 11,272,192 of 1,247,086,592 (0.90% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 20:03, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>0.952800</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.896900</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.855300</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.988100</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.853000</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.896100</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.909200</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.935500</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.791100</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.817000</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.895900</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.849100</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.794800</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.808700</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.884400</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.842600</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.886100</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.791600</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.817000</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.801400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [188/188 02:25]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loss for Trial 15: 0.8051513433456421\n","\n","--- Starting Trial 16 ---\n","Params: LR=3.54e-05, r=16, alpha=16, accum=2\n","==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["The model is already on multiple devices. Skipping the move to device specified in `args`.\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 74,250 | Num Epochs = 1 | Total steps = 100\n","O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 2\n","\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 2 x 1) = 16\n"," \"-____-\"     Trainable parameters = 11,272,192 of 1,247,086,592 (0.90% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 10:28, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>0.951200</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.925700</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.905800</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.954400</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.870300</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.845300</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.956100</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.084800</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.897500</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.895500</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.925300</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.930700</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.935500</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.948200</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.964800</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.985700</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.765600</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.881800</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.928700</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.780300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [188/188 02:25]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loss for Trial 16: 0.8537077903747559\n","\n","============================================================\n","FINAL RESULTS SUMMARY (Deterministic Fixed-Budget)\n","============================================================\n","\n","Best Eval Loss: 0.7998\n","\n","BEST HYPERPARAMETERS:\n","  - r: 16\n","  - alpha_ratio: 2\n","  - lora_alpha: 32\n","  - learning_rate: 1.46e-04\n","  - gradient_accumulation_steps: 8\n"]}],"source":["import os\n","import random\n","import gc\n","import torch\n","\n","from trl import SFTTrainer\n","from transformers import TrainingArguments, DataCollatorForSeq2Seq\n","from unsloth import FastLanguageModel, is_bfloat16_supported\n","from unsloth.chat_templates import train_on_responses_only\n","\n","# =========================================================\n","# 1. SETUP PARAMS AND DATA\n","# =========================================================\n","\n","# General Training Parameters\n","max_seq_length = 2048\n","load_in_4bit = True\n","dtype = torch.bfloat16 if is_bfloat16_supported() else torch.float16\n","\n","# Fixed Seed for overall training reproducibility\n","BASE_SEED = 3407\n","N_TRIALS = 16\n","FIXED_STEPS = 100\n","\n","# --- Dataset Loading ---\n","dataset_split = dataset.train_test_split(test_size=0.01, seed=BASE_SEED)\n","train_dataset = dataset_split[\"train\"]\n","eval_dataset = dataset_split[\"test\"]\n","\n","# =========================================================\n","# 2. DETERMINISTIC HYPERPARAMETER SAMPLING\n","# =========================================================\n","\n","def generate_deterministic_hparams(seed: int):\n","    \"\"\"\n","    Manually samples hyperparameters deterministically using a per-trial seed.\n","    This mimics Optuna's deterministic random sampler.\n","    \"\"\"\n","    rng = random.Random(seed)\n","\n","    # Categorical sampling\n","    r_value = rng.choice([8, 16, 32])\n","    alpha_ratio = rng.choice([1, 2])\n","    lora_alpha = r_value * alpha_ratio\n","    grad_accum = rng.choice([2, 4, 8])\n","\n","    # Log-Uniform sampling (Learning Rate)\n","    log_min = torch.log(torch.tensor(2e-5)).item()\n","    log_max = torch.log(torch.tensor(2e-4)).item()\n","    log_lr = rng.uniform(log_min, log_max)\n","    learning_rate = torch.exp(torch.tensor(log_lr)).item()\n","\n","    return {\n","        \"r\": r_value,\n","        \"alpha_ratio\": alpha_ratio,\n","        \"lora_alpha\": lora_alpha,\n","        \"learning_rate\": learning_rate,\n","        \"gradient_accumulation_steps\": grad_accum,\n","    }\n","\n","\n","# =========================================================\n","# 3. TRAINING FUNCTION (Fixed 100-step budget)\n","# =========================================================\n","\n","def run_training_trial(hparams, trial_number):\n","    \"\"\"Runs a single training session with a fixed budget.\"\"\"\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    r_value = hparams[\"r\"]\n","    lora_alpha = hparams[\"lora_alpha\"]\n","    learning_rate = hparams[\"learning_rate\"]\n","    grad_accum = hparams[\"gradient_accumulation_steps\"]\n","\n","    print(f\"\\n--- Starting Trial {trial_number} ---\")\n","    print(f\"Params: LR={learning_rate:.2e}, r={r_value}, alpha={lora_alpha}, accum={grad_accum}\")\n","\n","    # Build model (Unsloth optimized loading and PEFT-wrapping)\n","    base_model, tokenizer = FastLanguageModel.from_pretrained(\n","        model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n","        max_seq_length = max_seq_length,\n","        dtype = dtype,\n","        load_in_4bit = load_in_4bit,\n","    )\n","\n","    model = FastLanguageModel.get_peft_model(\n","        base_model,\n","        r=r_value,\n","        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n","        lora_alpha=lora_alpha,\n","        lora_dropout=0,\n","        bias=\"none\",\n","        use_gradient_checkpointing=\"unsloth\",\n","        random_state=BASE_SEED,\n","    )\n","\n","    # TrainingArguments\n","    trial_output_dir = os.path.join(checkpoint_dir, f\"trial_{trial_number}\")\n","    os.makedirs(trial_output_dir, exist_ok=True)\n","\n","    args = TrainingArguments(\n","        output_dir=trial_output_dir,\n","        per_device_train_batch_size=8,\n","        per_device_eval_batch_size=4,\n","        gradient_accumulation_steps=grad_accum,\n","        warmup_ratio=0.1,\n","        max_steps=FIXED_STEPS,\n","        learning_rate=learning_rate,\n","        fp16=not is_bfloat16_supported(),\n","        bf16=is_bfloat16_supported(),\n","        logging_steps=5,\n","        optim=\"adamw_8bit\",\n","        weight_decay=0.01,\n","        lr_scheduler_type=\"linear\",\n","        seed=BASE_SEED,\n","        report_to=\"none\",\n","        save_strategy='no',\n","    )\n","\n","    # SFTTrainer Initialization and Training\n","    trainer = SFTTrainer(\n","        model=model,\n","        tokenizer=tokenizer,\n","        train_dataset=train_dataset,\n","        eval_dataset=eval_dataset,\n","        dataset_text_field=\"text\",\n","        max_seq_length=max_seq_length,\n","        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n","        dataset_num_proc=10,\n","        packing=False,\n","        args=args,\n","    )\n","\n","    trainer = train_on_responses_only(\n","        trainer,\n","        instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n","        response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n","    )\n","\n","    trainer.train()\n","\n","    final_eval_loss = trainer.evaluate()[\"eval_loss\"]\n","\n","    # Clean up memory\n","    del model, base_model, trainer, tokenizer\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    return final_eval_loss\n","\n","\n","# =========================================================\n","# 4. DETERMINISTIC EXECUTION LOOP\n","# =========================================================\n","\n","results = []\n","best_loss = float('inf')\n","best_hparams = {}\n","\n","print(\"=\" * 60)\n","print(f\"Starting Deterministic Fixed-Budget Search: {N_TRIALS} Trials at {FIXED_STEPS} Steps Each\")\n","print(\"=\" * 60)\n","\n","for i in range(N_TRIALS):\n","    trial_number = i + 1\n","\n","    try:\n","        # Generate parameters using a unique, deterministic seed\n","        hparams = generate_deterministic_hparams(BASE_SEED + i)\n","\n","        # Run the fixed-budget training\n","        loss = run_training_trial(hparams, trial_number)\n","\n","        # Store and track results\n","        results.append({\"trial\": trial_number, \"loss\": loss, \"hparams\": hparams})\n","\n","        print(f\"Loss for Trial {trial_number}: {loss}\")\n","\n","        if loss < best_loss:\n","            best_loss = loss\n","            best_hparams = hparams\n","            print(f\"üèÜ NEW BEST LOSS FOUND: {best_loss:.4f} at Trial {trial_number}\")\n","\n","    except KeyboardInterrupt:\n","        print(\"\\nStopped manually. Saving current progress.\")\n","        break\n","    except Exception as e:\n","        print(f\"\\nTrial {trial_number} failed with error: {e}\")\n","        results.append({\"trial\": trial_number, \"loss\": float('nan'), \"hparams\": hparams})\n","        continue\n","\n","\n","# =========================================================\n","# 5. RESULTS SUMMARY\n","# =========================================================\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"FINAL RESULTS SUMMARY (Deterministic Fixed-Budget)\")\n","print(\"=\" * 60)\n","\n","if best_hparams:\n","    print(f\"\\nBest Eval Loss: {best_loss:.4f}\")\n","    print(\"\\nBEST HYPERPARAMETERS:\")\n","    for key, value in best_hparams.items():\n","        if key == \"learning_rate\":\n","             print(f\"  - {key}: {value:.2e}\")\n","        else:\n","             print(f\"  - {key}: {value}\")\n","else:\n","    print(\"\\nNo successful trials completed.\")"]},{"cell_type":"code","source":["from pprint import pprint\n","pprint(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Io-Ggi2MBxjE","executionInfo":{"status":"ok","timestamp":1764264734274,"user_tz":-60,"elapsed":4,"user":{"displayName":"Paul Kilian","userId":"12769555656558612045"}},"outputId":"747dc668-31c0-4b5b-dc46-402d726d6758"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'hparams': {'alpha_ratio': 2,\n","              'gradient_accumulation_steps': 2,\n","              'learning_rate': 4.284925671527162e-05,\n","              'lora_alpha': 16,\n","              'r': 8},\n","  'loss': 0.8473519682884216,\n","  'trial': 1},\n"," {'hparams': {'alpha_ratio': 2,\n","              'gradient_accumulation_steps': 4,\n","              'learning_rate': 3.403441223781556e-05,\n","              'lora_alpha': 16,\n","              'r': 8},\n","  'loss': 0.8476158976554871,\n","  'trial': 2},\n"," {'hparams': {'alpha_ratio': 2,\n","              'gradient_accumulation_steps': 2,\n","              'learning_rate': 2.240711728518363e-05,\n","              'lora_alpha': 16,\n","              'r': 8},\n","  'loss': 0.870863676071167,\n","  'trial': 3},\n"," {'hparams': {'alpha_ratio': 1,\n","              'gradient_accumulation_steps': 4,\n","              'learning_rate': 6.185057281982154e-05,\n","              'lora_alpha': 16,\n","              'r': 16},\n","  'loss': 0.830998957157135,\n","  'trial': 4},\n"," {'hparams': {'alpha_ratio': 2,\n","              'gradient_accumulation_steps': 2,\n","              'learning_rate': 0.00010820055467775092,\n","              'lora_alpha': 16,\n","              'r': 8},\n","  'loss': 0.8257739543914795,\n","  'trial': 5},\n"," {'hparams': {'alpha_ratio': 1,\n","              'gradient_accumulation_steps': 2,\n","              'learning_rate': 4.060842184117064e-05,\n","              'lora_alpha': 16,\n","              'r': 16},\n","  'loss': 0.849261462688446,\n","  'trial': 6},\n"," {'hparams': {'alpha_ratio': 1,\n","              'gradient_accumulation_steps': 4,\n","              'learning_rate': 3.807640678132884e-05,\n","              'lora_alpha': 16,\n","              'r': 16},\n","  'loss': 0.8444796800613403,\n","  'trial': 7},\n"," {'hparams': {'alpha_ratio': 2,\n","              'gradient_accumulation_steps': 2,\n","              'learning_rate': 4.5835578930564225e-05,\n","              'lora_alpha': 32,\n","              'r': 16},\n","  'loss': 0.8348084092140198,\n","  'trial': 8},\n"," {'hparams': {'alpha_ratio': 2,\n","              'gradient_accumulation_steps': 8,\n","              'learning_rate': 0.0001049246930051595,\n","              'lora_alpha': 32,\n","              'r': 16},\n","  'loss': 0.8058328628540039,\n","  'trial': 9},\n"," {'hparams': {'alpha_ratio': 1,\n","              'gradient_accumulation_steps': 8,\n","              'learning_rate': 0.00012154182331869379,\n","              'lora_alpha': 32,\n","              'r': 32},\n","  'loss': 0.8030117750167847,\n","  'trial': 10},\n"," {'hparams': {'alpha_ratio': 2,\n","              'gradient_accumulation_steps': 8,\n","              'learning_rate': 0.00014625844778493047,\n","              'lora_alpha': 32,\n","              'r': 16},\n","  'loss': 0.7997848987579346,\n","  'trial': 11},\n"," {'hparams': {'alpha_ratio': 1,\n","              'gradient_accumulation_steps': 2,\n","              'learning_rate': 2.6611900466377847e-05,\n","              'lora_alpha': 8,\n","              'r': 8},\n","  'loss': 0.8811953067779541,\n","  'trial': 12},\n"," {'hparams': {'alpha_ratio': 2,\n","              'gradient_accumulation_steps': 4,\n","              'learning_rate': 2.8778744308510795e-05,\n","              'lora_alpha': 16,\n","              'r': 8},\n","  'loss': 0.853173017501831,\n","  'trial': 13},\n"," {'hparams': {'alpha_ratio': 2,\n","              'gradient_accumulation_steps': 8,\n","              'learning_rate': 3.176930840709247e-05,\n","              'lora_alpha': 32,\n","              'r': 16},\n","  'loss': 0.8320459723472595,\n","  'trial': 14},\n"," {'hparams': {'alpha_ratio': 2,\n","              'gradient_accumulation_steps': 4,\n","              'learning_rate': 0.00016686950402799994,\n","              'lora_alpha': 32,\n","              'r': 16},\n","  'loss': 0.8051513433456421,\n","  'trial': 15},\n"," {'hparams': {'alpha_ratio': 1,\n","              'gradient_accumulation_steps': 2,\n","              'learning_rate': 3.543361890478991e-05,\n","              'lora_alpha': 16,\n","              'r': 16},\n","  'loss': 0.8537077903747559,\n","  'trial': 16}]\n"]}]},{"cell_type":"markdown","source":["### Fine-grained Search"],"metadata":{"id":"z-ZFFvisAzDE"}},{"cell_type":"code","source":["import os\n","import random\n","import gc\n","import torch\n","\n","from trl import SFTTrainer\n","from transformers import TrainingArguments, DataCollatorForSeq2Seq\n","from unsloth import FastLanguageModel, is_bfloat16_supported\n","from unsloth.chat_templates import train_on_responses_only\n","\n","# =========================================================\n","# 1. SETUP PARAMS AND DATA\n","# =========================================================\n","\n","# General Training Parameters\n","max_seq_length = 2048\n","load_in_4bit = True\n","dtype = torch.bfloat16 if is_bfloat16_supported() else torch.float16\n","\n","# Fixed Seed for overall training reproducibility\n","BASE_SEED = 3407\n","FIXED_STEPS = 300\n","\n","# --- Dataset Loading ---\n","dataset_split = dataset.train_test_split(test_size=0.01, seed=BASE_SEED)\n","train_dataset = dataset_split[\"train\"]\n","eval_dataset = dataset_split[\"test\"]\n","\n","# =========================================================\n","# 2. DETERMINISTIC HYPERPARAMETER SAMPLING\n","# =========================================================\n","\n","def generate_deterministic_hparams(seed: int):\n","    \"\"\"\n","    Manually samples hyperparameters deterministically using a per-trial seed.\n","    This mimics Optuna's deterministic random sampler.\n","    \"\"\"\n","    rng = random.Random(seed)\n","\n","    # Categorical sampling\n","    r_value = rng.choice([8, 16, 32])\n","    alpha_ratio = rng.choice([1, 2])\n","    lora_alpha = r_value * alpha_ratio\n","    grad_accum = rng.choice([2, 4, 8])\n","\n","    # Log-Uniform sampling (Learning Rate)\n","    log_min = torch.log(torch.tensor(2e-5)).item()\n","    log_max = torch.log(torch.tensor(2e-4)).item()\n","    log_lr = rng.uniform(log_min, log_max)\n","    learning_rate = torch.exp(torch.tensor(log_lr)).item()\n","\n","    return {\n","        \"r\": r_value,\n","        \"alpha_ratio\": alpha_ratio,\n","        \"lora_alpha\": lora_alpha,\n","        \"learning_rate\": learning_rate,\n","        \"gradient_accumulation_steps\": grad_accum,\n","    }\n","\n","\n","# =========================================================\n","# 3. TRAINING FUNCTION (Fixed 300-step budget)\n","# =========================================================\n","\n","def run_training_trial(hparams, trial_number):\n","    \"\"\"Runs a single training session with a fixed budget.\"\"\"\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    r_value = hparams[\"r\"]\n","    lora_alpha = hparams[\"lora_alpha\"]\n","    learning_rate = hparams[\"learning_rate\"]\n","    grad_accum = hparams[\"gradient_accumulation_steps\"]\n","\n","    print(f\"\\n--- Starting Trial {trial_number} ---\")\n","    print(f\"Params: LR={learning_rate:.2e}, r={r_value}, alpha={lora_alpha}, accum={grad_accum}\")\n","\n","    # Build model (Unsloth optimized loading and PEFT-wrapping)\n","    base_model, tokenizer = FastLanguageModel.from_pretrained(\n","        model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n","        max_seq_length = max_seq_length,\n","        dtype = dtype,\n","        load_in_4bit = load_in_4bit,\n","    )\n","\n","    model = FastLanguageModel.get_peft_model(\n","        base_model,\n","        r=r_value,\n","        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n","        lora_alpha=lora_alpha,\n","        lora_dropout=0,\n","        bias=\"none\",\n","        use_gradient_checkpointing=\"unsloth\",\n","        random_state=BASE_SEED,\n","    )\n","\n","    # TrainingArguments\n","    trial_output_dir = os.path.join(checkpoint_dir, f\"trial_{trial_number}\")\n","    os.makedirs(trial_output_dir, exist_ok=True)\n","\n","    args = TrainingArguments(\n","        output_dir=trial_output_dir,\n","        per_device_train_batch_size=8,\n","        per_device_eval_batch_size=4,\n","        gradient_accumulation_steps=grad_accum,\n","        warmup_ratio=0.1,\n","        max_steps=FIXED_STEPS,\n","        learning_rate=learning_rate,\n","        fp16=not is_bfloat16_supported(),\n","        bf16=is_bfloat16_supported(),\n","        logging_steps=5,\n","        optim=\"adamw_8bit\",\n","        weight_decay=0.01,\n","        lr_scheduler_type=\"linear\",\n","        seed=BASE_SEED,\n","        report_to=\"none\",\n","        save_strategy='no',\n","    )\n","\n","    # SFTTrainer Initialization and Training\n","    trainer = SFTTrainer(\n","        model=model,\n","        tokenizer=tokenizer,\n","        train_dataset=train_dataset,\n","        eval_dataset=eval_dataset,\n","        dataset_text_field=\"text\",\n","        max_seq_length=max_seq_length,\n","        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n","        dataset_num_proc=10,\n","        packing=False,\n","        args=args,\n","    )\n","\n","    trainer = train_on_responses_only(\n","        trainer,\n","        instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n","        response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n","    )\n","\n","    trainer.train()\n","\n","    final_eval_loss = trainer.evaluate()[\"eval_loss\"]\n","\n","    # Clean up memory\n","    del model, base_model, trainer, tokenizer\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    return final_eval_loss\n","\n","\n","# =========================================================\n","# 4. DETERMINISTIC EXECUTION LOOP\n","# =========================================================\n","\n","results_fine = []\n","best_loss = float('inf')\n","best_hparams = {}\n","\n","print(\"=\" * 60)\n","print(f\"Starting Deterministic Fixed-Budget Search: 4 Trials at {FIXED_STEPS} Steps Each\")\n","print(\"=\" * 60)\n","\n","for trial_number in [9, 10, 11, 15]:\n","\n","    try:\n","        # Generate parameters using a unique, deterministic seed\n","        hparams = generate_deterministic_hparams(BASE_SEED + (trial_number - 1))\n","        print(f\"\")\n","\n","        # Run the fixed-budget training\n","        loss = run_training_trial(hparams, trial_number)\n","\n","        # Store and track results\n","        results_fine.append({\"trial\": trial_number, \"loss\": loss, \"hparams\": hparams})\n","\n","        print(f\"Loss for Trial {trial_number}: {loss}\")\n","\n","        if loss < best_loss:\n","            best_loss = loss\n","            best_hparams = hparams\n","            print(f\"üèÜ NEW BEST LOSS FOUND: {best_loss:.4f} at Trial {trial_number}\")\n","\n","    except KeyboardInterrupt:\n","        print(\"\\nStopped manually. Saving current progress.\")\n","        break\n","    except Exception as e:\n","        print(f\"\\nTrial {trial_number} failed with error: {e}\")\n","        results_fine.append({\"trial\": trial_number, \"loss\": float('nan'), \"hparams\": hparams})\n","        continue\n","\n","\n","# =========================================================\n","# 5. RESULTS SUMMARY\n","# =========================================================\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"FINAL RESULTS SUMMARY (Deterministic Fixed-Budget)\")\n","print(\"=\" * 60)\n","\n","if best_hparams:\n","    print(f\"\\nBest Eval Loss: {best_loss:.4f}\")\n","    print(\"\\nBEST HYPERPARAMETERS:\")\n","    for key, value in best_hparams.items():\n","        if key == \"learning_rate\":\n","             print(f\"  - {key}: {value:.2e}\")\n","        else:\n","             print(f\"  - {key}: {value}\")\n","else:\n","    print(\"\\nNo successful trials completed.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["42f26d6fb7cc4a04b5dde2f58f1d846f","0c45d29147784671b34dc72f53585415","9d18d394b5444dd59fa06c7058c13d2d","4b326de340b04a5889d27d8788301f8e","e35d4dd2a5e94b0e80a4cfe1efd51889","f90a696698fc4ce6b45a7e23dac63ae3","948773f5269940ebaa8c872327ade283","4fb95022826a4e4fa7c65f3161a57971","1d9c90e9922745df83fa8222e745143e","bfcfaa642376426fb6c840f6c6db3a46","8cefdb8c0e334f9880fb4519327a63b6","8807b789b4d2492aa78bda071c8eed26","0adae36269f3477fb83a718aa9630e14","e6cb2c3cd2c04f988e329352298c4927","146076fcb8cb4a339c1ed899f2877356","b64db706e85c432cbf541c213c222322","9c9d5bff3f2c4db291d9d43fcb3438ef","a322e582ad2f4b64bf07863d7bb35ffc","d634d2067f8f4ef5b499eb9bd3141720","61bc931090e049b69b2519231cd1f666","c806c4eb182d4d1495657763ddb4a972","45ac4535b30346d091db76249fcba1a1","a5fb7a7740dc4672a8da8b814229e4db","44688006bd804ec1a51e4a1af7a3d5ea","0e0d6028be0943ebbdccdff30662f79c","70a7d5e8c8524286881535db106f57da","f12c578d84724b628b3ff1d8bf0e4392","e4ac8dc1e39a47db929c269caec8f3ee","6bc99aec78f34ad9bb6fcf18c8fde66f","cff67de4535f420ab48cb627ebb6d092","915eb8be2a524478874bc0e690020a37","408b7809ee044b09b8b51f138843581b","a8169759d87d4bd9bac1645add794efc","48dc12884a264b56b3363e1f60843200","0789c776d94c4f0d856f69b70907a674","d996f252223f46ee8efda42bd0fb1a5a","8727960e9e024e7986620824dc069f8a","0204b02b0fe74e9aa89b14d78f615911","0352ffd4f04e4d498ac4a6fa283b1496","aedb38decb7948f2b8154cbd871a4332","de5b2b0c3b4343458245be18792c8bd3","b853e58e043d49b489f800aff889475d","6f5e909465684feda7e762a3dc568b1b","55b978b08d1b43b4930c3c991832a100"]},"id":"HAT2nQroBDxi","executionInfo":{"status":"ok","timestamp":1764294488966,"user_tz":-60,"elapsed":24720544,"user":{"displayName":"Paul Kilian","userId":"12769555656558612045"}},"outputId":"15ef5e00-8d33-4553-8900-1c6a69daec4c"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Starting Deterministic Fixed-Budget Search: 4 Trials at 300 Steps Each\n","============================================================\n","\n","\n","--- Starting Trial 9 ---\n","Params: LR=1.05e-04, r=16, alpha=32, accum=8\n","==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth 2025.11.4 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"]},{"output_type":"display_data","data":{"text/plain":["Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/74250 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42f26d6fb7cc4a04b5dde2f58f1d846f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/750 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8807b789b4d2492aa78bda071c8eed26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map (num_proc=6):   0%|          | 0/74250 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5fb7a7740dc4672a8da8b814229e4db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map (num_proc=6):   0%|          | 0/750 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48dc12884a264b56b3363e1f60843200"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The model is already on multiple devices. Skipping the move to device specified in `args`.\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 74,250 | Num Epochs = 1 | Total steps = 300\n","O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n","\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n"," \"-____-\"     Trainable parameters = 11,272,192 of 1,247,086,592 (0.90% trained)\n"]},{"output_type":"stream","name":"stdout","text":["Unsloth: Will smartly offload gradients to save VRAM!\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [300/300 1:52:47, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>0.953000</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.990100</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.950500</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.973200</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.854900</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.913200</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.822700</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.883800</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.861800</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.819300</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.838300</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.940000</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.873000</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.828700</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.884000</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.835500</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.856100</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.833000</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.821200</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.829600</td>\n","    </tr>\n","    <tr>\n","      <td>105</td>\n","      <td>0.901300</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.871700</td>\n","    </tr>\n","    <tr>\n","      <td>115</td>\n","      <td>0.866400</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.845900</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>0.852600</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.831200</td>\n","    </tr>\n","    <tr>\n","      <td>135</td>\n","      <td>0.810800</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.782400</td>\n","    </tr>\n","    <tr>\n","      <td>145</td>\n","      <td>0.876800</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.813200</td>\n","    </tr>\n","    <tr>\n","      <td>155</td>\n","      <td>0.788600</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.884900</td>\n","    </tr>\n","    <tr>\n","      <td>165</td>\n","      <td>0.807900</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.810700</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>0.816700</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.830600</td>\n","    </tr>\n","    <tr>\n","      <td>185</td>\n","      <td>0.814300</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>0.849700</td>\n","    </tr>\n","    <tr>\n","      <td>195</td>\n","      <td>0.806900</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.885000</td>\n","    </tr>\n","    <tr>\n","      <td>205</td>\n","      <td>0.808100</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>0.813600</td>\n","    </tr>\n","    <tr>\n","      <td>215</td>\n","      <td>0.803100</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>0.804100</td>\n","    </tr>\n","    <tr>\n","      <td>225</td>\n","      <td>0.790200</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>0.781200</td>\n","    </tr>\n","    <tr>\n","      <td>235</td>\n","      <td>0.810900</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.850100</td>\n","    </tr>\n","    <tr>\n","      <td>245</td>\n","      <td>0.823100</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.849000</td>\n","    </tr>\n","    <tr>\n","      <td>255</td>\n","      <td>0.824000</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>0.812200</td>\n","    </tr>\n","    <tr>\n","      <td>265</td>\n","      <td>0.798100</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>0.839600</td>\n","    </tr>\n","    <tr>\n","      <td>275</td>\n","      <td>0.842000</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>0.829800</td>\n","    </tr>\n","    <tr>\n","      <td>285</td>\n","      <td>0.871200</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>0.877900</td>\n","    </tr>\n","    <tr>\n","      <td>295</td>\n","      <td>0.875900</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.838900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n","Using gradient accumulation will be very slightly less accurate.\n","Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [188/188 02:17]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loss for Trial 9: 0.777212381362915\n","üèÜ NEW BEST LOSS FOUND: 0.7772 at Trial 9\n","\n","\n","--- Starting Trial 10 ---\n","Params: LR=1.22e-04, r=32, alpha=32, accum=8\n","==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["The model is already on multiple devices. Skipping the move to device specified in `args`.\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 74,250 | Num Epochs = 1 | Total steps = 300\n","O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n","\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n"," \"-____-\"     Trainable parameters = 22,544,384 of 1,258,358,784 (1.79% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [300/300 1:53:54, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>0.952700</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.988100</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.946000</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.968100</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.850400</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.909400</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.820000</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.880800</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.858800</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.816400</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.835000</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.936900</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.869800</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.826000</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.881400</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.833200</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.853500</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.830100</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.818400</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.827100</td>\n","    </tr>\n","    <tr>\n","      <td>105</td>\n","      <td>0.898900</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.868800</td>\n","    </tr>\n","    <tr>\n","      <td>115</td>\n","      <td>0.864400</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.843300</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>0.850200</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.828700</td>\n","    </tr>\n","    <tr>\n","      <td>135</td>\n","      <td>0.808000</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.780300</td>\n","    </tr>\n","    <tr>\n","      <td>145</td>\n","      <td>0.874000</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.810600</td>\n","    </tr>\n","    <tr>\n","      <td>155</td>\n","      <td>0.786100</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.882300</td>\n","    </tr>\n","    <tr>\n","      <td>165</td>\n","      <td>0.805600</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.808300</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>0.814300</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.828300</td>\n","    </tr>\n","    <tr>\n","      <td>185</td>\n","      <td>0.811800</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>0.847000</td>\n","    </tr>\n","    <tr>\n","      <td>195</td>\n","      <td>0.804800</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.882400</td>\n","    </tr>\n","    <tr>\n","      <td>205</td>\n","      <td>0.805300</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>0.811200</td>\n","    </tr>\n","    <tr>\n","      <td>215</td>\n","      <td>0.800500</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>0.801600</td>\n","    </tr>\n","    <tr>\n","      <td>225</td>\n","      <td>0.787500</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>0.778500</td>\n","    </tr>\n","    <tr>\n","      <td>235</td>\n","      <td>0.808400</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.847400</td>\n","    </tr>\n","    <tr>\n","      <td>245</td>\n","      <td>0.820500</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.846500</td>\n","    </tr>\n","    <tr>\n","      <td>255</td>\n","      <td>0.821000</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>0.809400</td>\n","    </tr>\n","    <tr>\n","      <td>265</td>\n","      <td>0.795500</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>0.837300</td>\n","    </tr>\n","    <tr>\n","      <td>275</td>\n","      <td>0.839600</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>0.826600</td>\n","    </tr>\n","    <tr>\n","      <td>285</td>\n","      <td>0.868500</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>0.875300</td>\n","    </tr>\n","    <tr>\n","      <td>295</td>\n","      <td>0.873100</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.836200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [188/188 02:18]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loss for Trial 10: 0.7743467688560486\n","üèÜ NEW BEST LOSS FOUND: 0.7743 at Trial 10\n","\n","\n","--- Starting Trial 11 ---\n","Params: LR=1.46e-04, r=16, alpha=32, accum=8\n","==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["The model is already on multiple devices. Skipping the move to device specified in `args`.\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 74,250 | Num Epochs = 1 | Total steps = 300\n","O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n","\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n"," \"-____-\"     Trainable parameters = 11,272,192 of 1,247,086,592 (0.90% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [300/300 1:53:08, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>0.952200</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.985000</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.940500</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.962100</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.845700</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.904500</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.817200</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.877000</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.855200</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.813600</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.831800</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.933900</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.866700</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.823300</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.878600</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.830800</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.850900</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.826700</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.815700</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.824900</td>\n","    </tr>\n","    <tr>\n","      <td>105</td>\n","      <td>0.896800</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.866000</td>\n","    </tr>\n","    <tr>\n","      <td>115</td>\n","      <td>0.862200</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.841300</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>0.847700</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.826400</td>\n","    </tr>\n","    <tr>\n","      <td>135</td>\n","      <td>0.805500</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.777900</td>\n","    </tr>\n","    <tr>\n","      <td>145</td>\n","      <td>0.871300</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.808700</td>\n","    </tr>\n","    <tr>\n","      <td>155</td>\n","      <td>0.783900</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.880100</td>\n","    </tr>\n","    <tr>\n","      <td>165</td>\n","      <td>0.803400</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.805900</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>0.812300</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.826000</td>\n","    </tr>\n","    <tr>\n","      <td>185</td>\n","      <td>0.809600</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>0.845300</td>\n","    </tr>\n","    <tr>\n","      <td>195</td>\n","      <td>0.802400</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.880300</td>\n","    </tr>\n","    <tr>\n","      <td>205</td>\n","      <td>0.802800</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>0.808700</td>\n","    </tr>\n","    <tr>\n","      <td>215</td>\n","      <td>0.798500</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>0.799400</td>\n","    </tr>\n","    <tr>\n","      <td>225</td>\n","      <td>0.785300</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>0.776400</td>\n","    </tr>\n","    <tr>\n","      <td>235</td>\n","      <td>0.806200</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.845300</td>\n","    </tr>\n","    <tr>\n","      <td>245</td>\n","      <td>0.818100</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.844300</td>\n","    </tr>\n","    <tr>\n","      <td>255</td>\n","      <td>0.818700</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>0.807000</td>\n","    </tr>\n","    <tr>\n","      <td>265</td>\n","      <td>0.793100</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>0.834600</td>\n","    </tr>\n","    <tr>\n","      <td>275</td>\n","      <td>0.837400</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>0.824400</td>\n","    </tr>\n","    <tr>\n","      <td>285</td>\n","      <td>0.866100</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>0.873000</td>\n","    </tr>\n","    <tr>\n","      <td>295</td>\n","      <td>0.870800</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.834100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [188/188 02:17]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loss for Trial 11: 0.7720085978507996\n","üèÜ NEW BEST LOSS FOUND: 0.7720 at Trial 11\n","\n","\n","--- Starting Trial 15 ---\n","Params: LR=1.67e-04, r=16, alpha=32, accum=4\n","==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["The model is already on multiple devices. Skipping the move to device specified in `args`.\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 74,250 | Num Epochs = 1 | Total steps = 300\n","O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n","\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n"," \"-____-\"     Trainable parameters = 11,272,192 of 1,247,086,592 (0.90% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [300/300 56:57, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>0.958200</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.922100</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.880200</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.009900</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.870500</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.906900</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.916400</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.939300</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.794200</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.818500</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.896300</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.848800</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.794000</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.807700</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.881700</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.838900</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.881300</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.785000</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.810500</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.793700</td>\n","    </tr>\n","    <tr>\n","      <td>105</td>\n","      <td>0.771600</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.850900</td>\n","    </tr>\n","    <tr>\n","      <td>115</td>\n","      <td>0.927900</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.908400</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>0.872900</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.842400</td>\n","    </tr>\n","    <tr>\n","      <td>135</td>\n","      <td>0.776600</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.856700</td>\n","    </tr>\n","    <tr>\n","      <td>145</td>\n","      <td>0.886700</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.848900</td>\n","    </tr>\n","    <tr>\n","      <td>155</td>\n","      <td>0.853400</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.797800</td>\n","    </tr>\n","    <tr>\n","      <td>165</td>\n","      <td>0.860200</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.830300</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>0.829800</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.792000</td>\n","    </tr>\n","    <tr>\n","      <td>185</td>\n","      <td>0.838300</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>0.779000</td>\n","    </tr>\n","    <tr>\n","      <td>195</td>\n","      <td>0.791000</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.843000</td>\n","    </tr>\n","    <tr>\n","      <td>205</td>\n","      <td>0.907900</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>0.869800</td>\n","    </tr>\n","    <tr>\n","      <td>215</td>\n","      <td>0.871300</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>0.833200</td>\n","    </tr>\n","    <tr>\n","      <td>225</td>\n","      <td>0.893600</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>0.818200</td>\n","    </tr>\n","    <tr>\n","      <td>235</td>\n","      <td>0.864100</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.807300</td>\n","    </tr>\n","    <tr>\n","      <td>245</td>\n","      <td>0.832900</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.841800</td>\n","    </tr>\n","    <tr>\n","      <td>255</td>\n","      <td>0.814100</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>0.823800</td>\n","    </tr>\n","    <tr>\n","      <td>265</td>\n","      <td>0.795200</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>0.809400</td>\n","    </tr>\n","    <tr>\n","      <td>275</td>\n","      <td>0.797100</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>0.754300</td>\n","    </tr>\n","    <tr>\n","      <td>285</td>\n","      <td>0.849900</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>0.864700</td>\n","    </tr>\n","    <tr>\n","      <td>295</td>\n","      <td>0.740900</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.867200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [188/188 02:17]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loss for Trial 15: 0.7790784239768982\n","\n","============================================================\n","FINAL RESULTS SUMMARY (Deterministic Fixed-Budget)\n","============================================================\n","\n","Best Eval Loss: 0.7720\n","\n","BEST HYPERPARAMETERS:\n","  - r: 16\n","  - alpha_ratio: 2\n","  - lora_alpha: 32\n","  - learning_rate: 1.46e-04\n","  - gradient_accumulation_steps: 8\n"]}]},{"cell_type":"code","source":["from pprint import pprint\n","pprint(results_fine)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fSrMHuNkJvfV","executionInfo":{"status":"ok","timestamp":1764294639270,"user_tz":-60,"elapsed":5,"user":{"displayName":"Paul Kilian","userId":"12769555656558612045"}},"outputId":"8b2439b5-a7e0-4bb6-aa4f-f8b8d3c8ad6e"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'hparams': {'alpha_ratio': 2,\n","              'gradient_accumulation_steps': 8,\n","              'learning_rate': 0.0001049246930051595,\n","              'lora_alpha': 32,\n","              'r': 16},\n","  'loss': 0.777212381362915,\n","  'trial': 9},\n"," {'hparams': {'alpha_ratio': 1,\n","              'gradient_accumulation_steps': 8,\n","              'learning_rate': 0.00012154182331869379,\n","              'lora_alpha': 32,\n","              'r': 32},\n","  'loss': 0.7743467688560486,\n","  'trial': 10},\n"," {'hparams': {'alpha_ratio': 2,\n","              'gradient_accumulation_steps': 8,\n","              'learning_rate': 0.00014625844778493047,\n","              'lora_alpha': 32,\n","              'r': 16},\n","  'loss': 0.7720085978507996,\n","  'trial': 11},\n"," {'hparams': {'alpha_ratio': 2,\n","              'gradient_accumulation_steps': 4,\n","              'learning_rate': 0.00016686950402799994,\n","              'lora_alpha': 32,\n","              'r': 16},\n","  'loss': 0.7790784239768982,\n","  'trial': 15}]\n"]}]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"FA4Fpc7XaC3a"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"95_Nn-89DhsL","colab":{"base_uri":"https://localhost:8080/","height":364},"outputId":"7b01db12-cba1-4562-8e89-258ca1f5b5f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["The model is already on multiple devices. Skipping the move to device specified in `args`.\n"]},{"output_type":"stream","name":"stdout","text":["No checkpoint found. Starting fresh.\n"]},{"output_type":"stream","name":"stderr","text":["==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 74,250 | Num Epochs = 1 | Total steps = 1,161\n","O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n","\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n"," \"-____-\"     Trainable parameters = 11,272,192 of 1,247,086,592 (0.90% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='142' max='1161' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 142/1161 53:06 < 6:26:32, 0.04 it/s, Epoch 0.12/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>50</td>\n","      <td>0.925500</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.864700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}],"source":["import os\n","import random\n","import gc\n","import torch\n","\n","from trl import SFTTrainer\n","from transformers import TrainingArguments, DataCollatorForSeq2Seq\n","from unsloth import FastLanguageModel, is_bfloat16_supported\n","from unsloth.chat_templates import train_on_responses_only\n","\n","# =========================================================\n","# 1. SETUP PARAMS AND DATA\n","# =========================================================\n","\n","# General Training Parameters\n","max_seq_length = 2048\n","load_in_4bit = True\n","dtype = torch.bfloat16 if is_bfloat16_supported() else torch.float16\n","\n","# Fixed Seed for overall training reproducibility\n","BASE_SEED = 3407\n","\n","# --- Dataset Loading ---\n","dataset_split = dataset.train_test_split(test_size=0.01, seed=BASE_SEED)\n","train_dataset = dataset_split[\"train\"]\n","eval_dataset = dataset_split[\"test\"]\n","\n","\n","# =========================================================\n","# 2. DEFINE MODEL\n","# =========================================================\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","r_value = 16\n","lora_alpha = 32\n","learning_rate = 1.46e-04\n","grad_accum = 8\n","\n","# Build model (Unsloth optimized loading and PEFT-wrapping)\n","base_model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n",")\n","\n","model = FastLanguageModel.get_peft_model(\n","    base_model,\n","    r=r_value,\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n","    lora_alpha=lora_alpha,\n","    lora_dropout=0,\n","    bias=\"none\",\n","    use_gradient_checkpointing=\"unsloth\",\n","    random_state=BASE_SEED,\n",")\n","\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    tokenizer=tokenizer,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n","    dataset_num_proc=10,\n","    packing=False,\n","    args=TrainingArguments(\n","        per_device_train_batch_size=8,\n","        per_device_eval_batch_size=4,\n","        gradient_accumulation_steps=grad_accum,\n","        warmup_ratio=0.1,\n","        learning_rate=learning_rate,\n","        fp16=not is_bfloat16_supported(),\n","        bf16=is_bfloat16_supported(),\n","        logging_steps=50,\n","        optim=\"adamw_8bit\",\n","        weight_decay=0.01,\n","        lr_scheduler_type=\"linear\",\n","        seed=BASE_SEED,\n","        num_train_epochs = 1,\n","        save_strategy='steps',\n","        save_steps=100,\n","        save_total_limit=3,\n","        output_dir=checkpoint_dir,\n","        gradient_checkpointing=True,\n","        report_to = \"none\",\n","    ),\n",")\n","\n","trainer = train_on_responses_only(\n","    trainer,\n","    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n","    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",")\n","\n","checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint')]\n","checkpoints.sort(key=lambda x: int(x.split('-')[-1]))\n","if checkpoints:\n","    latest_checkpoint = os.path.join(checkpoint_dir, checkpoints[-1])\n","    print('Resuming from:', latest_checkpoint)\n","else:\n","    latest_checkpoint = None\n","    print('No checkpoint found. Starting fresh.')\n","trainer_stats = trainer.train(resume_from_checkpoint=latest_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ejIt2xSNKKp"},"outputs":[],"source":["#@title Show current memory stats\n","gpu_stats = torch.cuda.get_device_properties(0)\n","start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n","print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n","print(f\"{start_gpu_memory} GB of memory reserved.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pCqnaKmlO1U9"},"outputs":[],"source":["#@title Show final memory and time stats\n","used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n","used_percentage = round(used_memory         /max_memory*100, 3)\n","lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n","print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n","print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n","print(f\"Peak reserved memory = {used_memory} GB.\")\n","print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n","print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n","print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"]},{"cell_type":"markdown","metadata":{"id":"ekOmTR1hSNcr"},"source":["<a name=\"Inference\"></a>\n","### Inference\n","Let's run the model! You can change the instruction and input - leave the output blank!\n","\n","**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**\n","\n","We use `min_p = 0.1` and `temperature = 1.5`. Read this [Tweet](https://x.com/menhguin/status/1826132708508213629) for more information on why."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kR3gIAX-SM2q"},"outputs":[],"source":["from unsloth.chat_templates import get_chat_template\n","\n","tokenizer = get_chat_template(\n","    tokenizer,\n","    chat_template = \"llama-3.1\",\n",")\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","\n","messages = [\n","    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n","]\n","inputs = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize = True,\n","    add_generation_prompt = True, # Must add for generation\n","    return_tensors = \"pt\",\n",").to(\"cuda\")\n","\n","outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n","                         temperature = 1.5, min_p = 0.1)\n","tokenizer.batch_decode(outputs)"]},{"cell_type":"markdown","metadata":{"id":"CrSvZObor0lY"},"source":[" You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e2pEuRb1r2Vg"},"outputs":[],"source":["FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","\n","messages = [\n","    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n","]\n","inputs = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize = True,\n","    add_generation_prompt = True, # Must add for generation\n","    return_tensors = \"pt\",\n",").to(\"cuda\")\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n","_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n","                   use_cache = True, temperature = 1.5, min_p = 0.1)"]},{"cell_type":"markdown","metadata":{"id":"uMuVrWbjAzhc"},"source":["<a name=\"Save\"></a>\n","### Saving, loading finetuned models\n","To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n","\n","**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"upcOlWe7A1vc"},"outputs":[],"source":["model.save_pretrained(\"lora_model\") # Local saving\n","tokenizer.save_pretrained(\"lora_model\")\n","# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n","# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"]},{"cell_type":"markdown","metadata":{"id":"AEEcJ4qfC7Lp"},"source":["Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MKX_XKs_BNZR"},"outputs":[],"source":["if False:\n","    from unsloth import FastLanguageModel\n","    model, tokenizer = FastLanguageModel.from_pretrained(\n","        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n","        max_seq_length = max_seq_length,\n","        dtype = dtype,\n","        load_in_4bit = load_in_4bit,\n","    )\n","    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","\n","messages = [\n","    {\"role\": \"user\", \"content\": \"Describe a tall tower in the capital of France.\"},\n","]\n","inputs = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize = True,\n","    add_generation_prompt = True, # Must add for generation\n","    return_tensors = \"pt\",\n",").to(\"cuda\")\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n","_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n","                   use_cache = True, temperature = 1.5, min_p = 0.1)"]},{"cell_type":"markdown","metadata":{"id":"QQMjaNrjsU5_"},"source":["You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yFfaXG0WsQuE"},"outputs":[],"source":["if False:\n","    # I highly do NOT suggest - use Unsloth if possible\n","    from peft import AutoPeftModelForCausalLM\n","    from transformers import AutoTokenizer\n","    model = AutoPeftModelForCausalLM.from_pretrained(\n","        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n","        load_in_4bit = load_in_4bit,\n","    )\n","    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"]},{"cell_type":"markdown","metadata":{"id":"f422JgM9sdVT"},"source":["### Saving to float16 for VLLM\n","\n","We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHjt_SMYsd3P"},"outputs":[],"source":["# Merge to 16bit\n","if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n","if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n","\n","# Merge to 4bit\n","if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n","if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n","\n","# Just LoRA adapters\n","if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n","if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"]},{"cell_type":"markdown","metadata":{"id":"TCv4vXHd61i7"},"source":["### GGUF / llama.cpp Conversion\n","To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n","\n","Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n","* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n","* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n","* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n","\n","[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FqfebeAdT073"},"outputs":[],"source":["# Save to 8bit Q8_0\n","if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n","# Remember to go to https://huggingface.co/settings/tokens for a token!\n","# And change hf to your username!\n","if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n","\n","# Save to 16bit GGUF\n","if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n","if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n","\n","# Save to q4_k_m GGUF\n","if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n","if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n","\n","# Save to multiple GGUF options - much faster if you want multiple!\n","if False:\n","    model.push_to_hub_gguf(\n","        \"hf/model\", # Change hf to your username!\n","        tokenizer,\n","        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n","        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n","    )"]},{"cell_type":"markdown","metadata":{"id":"bDp0zNpwe6U_"},"source":["Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in `llama.cpp` or a UI based system like `GPT4All`. You can install GPT4All by going [here](https://gpt4all.io/index.html).\n","\n","**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**"]},{"cell_type":"markdown","metadata":{"id":"Zt9CHJqO6p30"},"source":["And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n","\n","Some other links:\n","1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)\n","2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)\n","3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)\n","4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)\n","5. Mistral 7b [free Kaggle version](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\n","6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with ü§ó HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!\n","7. `ChatML` for ShareGPT datasets, [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)\n","8. Text completions like novel writing [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)\n","9. [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)\n","10. [**NEW**] We make Gemma-2 9b / 27b **2x faster**! See our [Gemma-2 9b notebook](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)\n","11. [**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)\n","12. [**NEW**] We make Mistral NeMo 12B 2x faster and fit in under 12GB of VRAM! [Mistral NeMo notebook](https://colab.research.google.com/drive/17d3U-CAIwzmbDRqbZ9NnpHxCkmXB6LZ0?usp=sharing)\n","\n","<div class=\"align-center\">\n","  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n","  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n","  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Support our work if you can! Thanks!\n","</div>"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/bakalis/llm_fine_tuning/blob/main/llama_fine_tuning.ipynb","timestamp":1764176750069}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"42f26d6fb7cc4a04b5dde2f58f1d846f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0c45d29147784671b34dc72f53585415","IPY_MODEL_9d18d394b5444dd59fa06c7058c13d2d","IPY_MODEL_4b326de340b04a5889d27d8788301f8e"],"layout":"IPY_MODEL_e35d4dd2a5e94b0e80a4cfe1efd51889"}},"0c45d29147784671b34dc72f53585415":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f90a696698fc4ce6b45a7e23dac63ae3","placeholder":"‚Äã","style":"IPY_MODEL_948773f5269940ebaa8c872327ade283","value":"Unsloth:‚ÄáTokenizing‚Äá[&quot;text&quot;]‚Äá(num_proc=6):‚Äá100%"}},"9d18d394b5444dd59fa06c7058c13d2d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fb95022826a4e4fa7c65f3161a57971","max":74250,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1d9c90e9922745df83fa8222e745143e","value":74250}},"4b326de340b04a5889d27d8788301f8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bfcfaa642376426fb6c840f6c6db3a46","placeholder":"‚Äã","style":"IPY_MODEL_8cefdb8c0e334f9880fb4519327a63b6","value":"‚Äá74250/74250‚Äá[02:11&lt;00:00,‚Äá978.30‚Äáexamples/s]"}},"e35d4dd2a5e94b0e80a4cfe1efd51889":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f90a696698fc4ce6b45a7e23dac63ae3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"948773f5269940ebaa8c872327ade283":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4fb95022826a4e4fa7c65f3161a57971":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d9c90e9922745df83fa8222e745143e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bfcfaa642376426fb6c840f6c6db3a46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8cefdb8c0e334f9880fb4519327a63b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8807b789b4d2492aa78bda071c8eed26":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0adae36269f3477fb83a718aa9630e14","IPY_MODEL_e6cb2c3cd2c04f988e329352298c4927","IPY_MODEL_146076fcb8cb4a339c1ed899f2877356"],"layout":"IPY_MODEL_b64db706e85c432cbf541c213c222322"}},"0adae36269f3477fb83a718aa9630e14":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c9d5bff3f2c4db291d9d43fcb3438ef","placeholder":"‚Äã","style":"IPY_MODEL_a322e582ad2f4b64bf07863d7bb35ffc","value":"Unsloth:‚ÄáTokenizing‚Äá[&quot;text&quot;]‚Äá(num_proc=6):‚Äá100%"}},"e6cb2c3cd2c04f988e329352298c4927":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d634d2067f8f4ef5b499eb9bd3141720","max":750,"min":0,"orientation":"horizontal","style":"IPY_MODEL_61bc931090e049b69b2519231cd1f666","value":750}},"146076fcb8cb4a339c1ed899f2877356":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c806c4eb182d4d1495657763ddb4a972","placeholder":"‚Äã","style":"IPY_MODEL_45ac4535b30346d091db76249fcba1a1","value":"‚Äá750/750‚Äá[00:07&lt;00:00,‚Äá155.21‚Äáexamples/s]"}},"b64db706e85c432cbf541c213c222322":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c9d5bff3f2c4db291d9d43fcb3438ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a322e582ad2f4b64bf07863d7bb35ffc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d634d2067f8f4ef5b499eb9bd3141720":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61bc931090e049b69b2519231cd1f666":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c806c4eb182d4d1495657763ddb4a972":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45ac4535b30346d091db76249fcba1a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5fb7a7740dc4672a8da8b814229e4db":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_44688006bd804ec1a51e4a1af7a3d5ea","IPY_MODEL_0e0d6028be0943ebbdccdff30662f79c","IPY_MODEL_70a7d5e8c8524286881535db106f57da"],"layout":"IPY_MODEL_f12c578d84724b628b3ff1d8bf0e4392"}},"44688006bd804ec1a51e4a1af7a3d5ea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4ac8dc1e39a47db929c269caec8f3ee","placeholder":"‚Äã","style":"IPY_MODEL_6bc99aec78f34ad9bb6fcf18c8fde66f","value":"Map‚Äá(num_proc=6):‚Äá100%"}},"0e0d6028be0943ebbdccdff30662f79c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cff67de4535f420ab48cb627ebb6d092","max":74250,"min":0,"orientation":"horizontal","style":"IPY_MODEL_915eb8be2a524478874bc0e690020a37","value":74250}},"70a7d5e8c8524286881535db106f57da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_408b7809ee044b09b8b51f138843581b","placeholder":"‚Äã","style":"IPY_MODEL_a8169759d87d4bd9bac1645add794efc","value":"‚Äá74250/74250‚Äá[00:30&lt;00:00,‚Äá4107.34‚Äáexamples/s]"}},"f12c578d84724b628b3ff1d8bf0e4392":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4ac8dc1e39a47db929c269caec8f3ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bc99aec78f34ad9bb6fcf18c8fde66f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cff67de4535f420ab48cb627ebb6d092":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"915eb8be2a524478874bc0e690020a37":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"408b7809ee044b09b8b51f138843581b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8169759d87d4bd9bac1645add794efc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"48dc12884a264b56b3363e1f60843200":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0789c776d94c4f0d856f69b70907a674","IPY_MODEL_d996f252223f46ee8efda42bd0fb1a5a","IPY_MODEL_8727960e9e024e7986620824dc069f8a"],"layout":"IPY_MODEL_0204b02b0fe74e9aa89b14d78f615911"}},"0789c776d94c4f0d856f69b70907a674":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0352ffd4f04e4d498ac4a6fa283b1496","placeholder":"‚Äã","style":"IPY_MODEL_aedb38decb7948f2b8154cbd871a4332","value":"Map‚Äá(num_proc=6):‚Äá100%"}},"d996f252223f46ee8efda42bd0fb1a5a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_de5b2b0c3b4343458245be18792c8bd3","max":750,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b853e58e043d49b489f800aff889475d","value":750}},"8727960e9e024e7986620824dc069f8a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f5e909465684feda7e762a3dc568b1b","placeholder":"‚Äã","style":"IPY_MODEL_55b978b08d1b43b4930c3c991832a100","value":"‚Äá750/750‚Äá[00:00&lt;00:00,‚Äá234.89‚Äáexamples/s]"}},"0204b02b0fe74e9aa89b14d78f615911":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0352ffd4f04e4d498ac4a6fa283b1496":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aedb38decb7948f2b8154cbd871a4332":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de5b2b0c3b4343458245be18792c8bd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b853e58e043d49b489f800aff889475d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6f5e909465684feda7e762a3dc568b1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55b978b08d1b43b4930c3c991832a100":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}